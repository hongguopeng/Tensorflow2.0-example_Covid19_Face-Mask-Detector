{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths = []\n",
    "for files in os.listdir('./dataset'):\n",
    "    for image in os.listdir('./dataset/{}'.format(files)):\n",
    "        # os.path.splitext(image) => 獲取image的副檔名\n",
    "        imagePaths.append('./dataset/{}/{}'.format(files , image))\n",
    "\n",
    "# 獲取數據標簽\n",
    "data , labels = [] , []\n",
    "for imagePath in imagePaths:\n",
    "    # 讀取image，並將image做resize\n",
    "    image = load_img(imagePath, target_size = (224 , 224))\n",
    "    image = img_to_array(image)\n",
    "    image = preprocess_input(image)\n",
    "    data.append(image)\n",
    "\n",
    "    if imagePath.split('/')[2] == 'without_mask':\n",
    "        labels.append([0 , 1])\n",
    "    if imagePath.split('/')[2] == 'with_mask':\n",
    "        labels.append([1 , 0])\n",
    "data = np.array(data , dtype = 'float32')\n",
    "labels = np.array(labels).astype(np.float32)\n",
    "\n",
    "# 訓練集與測試集切分\n",
    "trainX , testX , trainY , testY =\\\n",
    "train_test_split(data , labels , test_size = 0.20 , random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune with MobileNetV2\n",
    "mobilenet = MobileNetV2(weights = 'imagenet' ,\n",
    "                        include_top = False,\n",
    "                        input_tensor = Input(shape = (224 , 224 , 3)))\n",
    "\n",
    "\n",
    "# 自定義layer的寫法，在本程式中用來替代tf.keras.layers.Dense\n",
    "class add_layer(tf.keras.layers.Layer):   \n",
    "    def __init__(self, in_size , out_size , name_ , activation_function = None):\n",
    "        super(add_layer , self).__init__()\n",
    "        initializer = tf.initializers.GlorotUniform()\n",
    "        self.Weights = tf.Variable(initial_value = initializer([in_size , out_size]) , name = '{}_w'.format(name_))\n",
    "        self.biases = tf.Variable(tf.zeros([1 , out_size]) + 0.00001 , name = '{}_b'.format(name_))\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self.activation_function is None:\n",
    "            return tf.matmul(inputs , self.Weights) + self.biases\n",
    "        else:\n",
    "            return self.activation_function(tf.matmul(inputs , self.Weights) + self.biases)\n",
    "\n",
    "\n",
    "class new_layer(tf.Module):\n",
    "    def __init__(self , name = None):\n",
    "        super(new_layer , self).__init__(name = name)\n",
    "        with self.name_scope: #相當於with tf.name_scope('new_layer')      \n",
    "            self.maxpooling = tf.keras.layers.AveragePooling2D(pool_size = (7 , 7))\n",
    "            self.flatten = tf.keras.layers.Flatten()\n",
    "            self.add_layer_1 = add_layer(1280 , 128 , 'layer_1' , tf.nn.relu)\n",
    "            self.add_layer_2 = add_layer(128 , 2 , 'layer_2')\n",
    "            self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "       \n",
    "    def __call__(self , inputs , training = True):           \n",
    "        x = self.maxpooling(inputs)          \n",
    "        x = self.flatten(x) \n",
    "        x = self.add_layer_1(x)\n",
    "        x = self.dropout(x , training = training)\n",
    "        x = self.add_layer_2(x)   \n",
    "        output = tf.nn.softmax(x , axis = 1)\n",
    "        prediction = tf.math.log(tf.clip_by_value(output , 1e-8 , tf.reduce_max(output)))\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "epoch_i : 0\n",
      "batch_i : 0\n",
      "training_loss : 0.63\n",
      "training_accuracy : 53.12%\n",
      "==============================\n",
      "epoch_i : 0\n",
      "batch_i : 5\n",
      "training_loss : 0.47\n",
      "training_accuracy : 81.25%\n",
      "==============================\n",
      "epoch_i : 0\n",
      "batch_i : 10\n",
      "training_loss : 0.25\n",
      "training_accuracy : 93.75%\n",
      "==============================\n",
      "epoch_i : 0\n",
      "batch_i : 15\n",
      "training_loss : 0.32\n",
      "training_accuracy : 87.50%\n",
      "==============================\n",
      "epoch_i : 0\n",
      "batch_i : 20\n",
      "training_loss : 0.16\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 0\n",
      "batch_i : 25\n",
      "training_loss : 0.10\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 0\n",
      "batch_i : 30\n",
      "training_loss : 0.09\n",
      "training_accuracy : 96.88%\n",
      "******************************\n",
      "epoch_i : 0\n",
      "testing_accuracy : 0.06\n",
      "testing_accuracy : 100.00%\n",
      "\n",
      "==============================\n",
      "epoch_i : 1\n",
      "batch_i : 0\n",
      "training_loss : 0.07\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 1\n",
      "batch_i : 5\n",
      "training_loss : 0.05\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 1\n",
      "batch_i : 10\n",
      "training_loss : 0.13\n",
      "training_accuracy : 96.88%\n",
      "==============================\n",
      "epoch_i : 1\n",
      "batch_i : 15\n",
      "training_loss : 0.05\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 1\n",
      "batch_i : 20\n",
      "training_loss : 0.03\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 1\n",
      "batch_i : 25\n",
      "training_loss : 0.02\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 1\n",
      "batch_i : 30\n",
      "training_loss : 0.03\n",
      "training_accuracy : 100.00%\n",
      "******************************\n",
      "epoch_i : 1\n",
      "testing_accuracy : 0.03\n",
      "testing_accuracy : 100.00%\n",
      "\n",
      "==============================\n",
      "epoch_i : 2\n",
      "batch_i : 0\n",
      "training_loss : 0.03\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 2\n",
      "batch_i : 5\n",
      "training_loss : 0.02\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 2\n",
      "batch_i : 10\n",
      "training_loss : 0.03\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 2\n",
      "batch_i : 15\n",
      "training_loss : 0.02\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 2\n",
      "batch_i : 20\n",
      "training_loss : 0.07\n",
      "training_accuracy : 96.88%\n",
      "==============================\n",
      "epoch_i : 2\n",
      "batch_i : 25\n",
      "training_loss : 0.04\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 2\n",
      "batch_i : 30\n",
      "training_loss : 0.02\n",
      "training_accuracy : 100.00%\n",
      "******************************\n",
      "epoch_i : 2\n",
      "testing_accuracy : 0.02\n",
      "testing_accuracy : 100.00%\n",
      "\n",
      "==============================\n",
      "epoch_i : 3\n",
      "batch_i : 0\n",
      "training_loss : 0.05\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 3\n",
      "batch_i : 5\n",
      "training_loss : 0.04\n",
      "training_accuracy : 96.88%\n",
      "==============================\n",
      "epoch_i : 3\n",
      "batch_i : 10\n",
      "training_loss : 0.03\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 3\n",
      "batch_i : 15\n",
      "training_loss : 0.03\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 3\n",
      "batch_i : 20\n",
      "training_loss : 0.00\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 3\n",
      "batch_i : 25\n",
      "training_loss : 0.01\n",
      "training_accuracy : 100.00%\n",
      "==============================\n",
      "epoch_i : 3\n",
      "batch_i : 30\n",
      "training_loss : 0.00\n",
      "training_accuracy : 100.00%\n",
      "******************************\n",
      "epoch_i : 3\n",
      "testing_accuracy : 0.01\n",
      "testing_accuracy : 100.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mask_model = new_layer()  \n",
    "\n",
    "# 對image做augmentation，防止overfitting\n",
    "aug = ImageDataGenerator(rotation_range = 25,\n",
    "                         width_shift_range = 0.15,\n",
    "                         height_shift_range = 0.2 ,\n",
    "                         shear_range = 0.2 ,\n",
    "                         zoom_range = 0.15,\n",
    "                         horizontal_flip = True ,\n",
    "                         fill_mode = 'nearest')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "\n",
    "@tf.function \n",
    "def train_step(x , y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        mobilenet_output = mobilenet(x)\n",
    "        prediction = mask_model(mobilenet_output , training = True)     \n",
    "        cross_entropy_temp = -tf.reduce_sum(y * prediction , axis = 1)\n",
    "        cross_entropy = tf.reduce_mean(cross_entropy_temp)\n",
    "        correct = tf.equal(tf.math.argmax(prediction , 1) , tf.argmax(y , 1))\n",
    "        correct = tf.cast(correct , tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct)\n",
    "    # 只訓練new_layer的variable，而MobileNetV2的variable則不更新\n",
    "    var_list = [var for var in mask_model.trainable_variables]\n",
    "    grads = tape.gradient(cross_entropy , var_list)   \n",
    "    optimizer.apply_gradients(grads_and_vars = zip(grads , var_list))\n",
    "    return cross_entropy , accuracy\n",
    "\n",
    "@tf.function\n",
    "def test_step(x , y):\n",
    "    mobilenet_output = mobilenet(x)\n",
    "    prediction = mask_model(mobilenet_output , training = False)     \n",
    "    cross_entropy_temp = -tf.reduce_sum(y * prediction , axis = 1)\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy_temp)\n",
    "    correct = tf.equal(tf.math.argmax(prediction , 1) , tf.argmax(y , 1))\n",
    "    correct = tf.cast(correct , tf.float32)\n",
    "    accuracy = tf.reduce_mean(correct)\n",
    "    return cross_entropy , accuracy\n",
    "    \n",
    "for epoch_i in range(0 , 4):\n",
    "    batches = 0\n",
    "    for batch_i , (x_batch , y_batch) in enumerate(aug.flow(trainX , trainY , shuffle = True , batch_size = 32)):\n",
    "        train_loss , train_acc = train_step(x_batch , y_batch)\n",
    "             \n",
    "        batches += 1\n",
    "        if batches >= len(trainX) / 32: break\n",
    "    \n",
    "        if batch_i % 5 == 0:\n",
    "            print('=' * 30)\n",
    "            print('epoch_i : {}'.format(epoch_i))\n",
    "            print('batch_i : {}'.format(batch_i))\n",
    "            print('training_loss : {:.2f}'.format(train_loss.numpy()))\n",
    "            print('training_accuracy : {:.2%}'.format(train_acc.numpy()))\n",
    "       \n",
    "    test_loss , test_acc = test_step(testX , testY)\n",
    "    print('*' * 30)\n",
    "    print('epoch_i : {}'.format(epoch_i))\n",
    "    print('testing_accuracy : {:.2f}'.format(test_loss.numpy()))\n",
    "    print('testing_accuracy : {:.2%}\\n'.format(test_acc.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\peng\\Anaconda3\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: my_network\\assets\n"
     ]
    }
   ],
   "source": [
    "# 模型存檔\n",
    "tf.saved_model.save(mask_model , 'my_network')\n",
    "sess = tf.compat.v1.Session()\n",
    "tf.io.write_graph(sess.graph , './my_network' , 'mask_model.pbtxt')\n",
    "restore_model = tf.saved_model.load('my_network')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
